# This line is required. It pulls in default overrides from the embedded cromwell `application.conf` needed for proper
# performance of cromwell.
include required("application")

// system.max-concurrent-workflows = 20

backend {
  default = "slurm"
  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"                                                                                     
      config {
        runtime-attributes = """
        Int runtime_minutes = 600
        String pbs_walltime = "24:00"
        Int cpus = 2
        Int pbs_cpu = 1
        Int requested_memory_mb_per_core = 8000
        Int memory_mb = 1000
        String? docker
        """

        submit = """
            sbatch \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${pbs_walltime} \
              ${"-c " + pbs_cpu} \
              --mem=${memory_mb} \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Ensure singularity is loaded if it's installed as a module
            #module load Singularity/3.0.1

            # Build the Docker image into a singularity image
            IMAGE=/SINGULARITY_IMAGES/${docker}.sif
            #IMAGE=${cwd}/${docker}.sif
            #singularity build $IMAGE docker://${docker}

            # Submit the script to SLURM
            sbatch \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${pbs_walltime} \
              ${"-c " + pbs_cpu} \
              --mem=${memory_mb} \
              --wrap "singularity exec -C --bind /EXAMPLE_DATA:/EXAMPLE_DATA:ro,${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
        concurrent-job-limit = 25
      }
    }


    PBS {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
      
// pbs_cpu is currently a String so that it can be evaluated as expression in runtime block
// https://github.com/broadinstitute/cromwell/issues/1702
        runtime-attributes = """
        Int memory_mb = 1000
        String  pbs_cpu = "1"
        String? pbs_email
        String? pbs_queue
        String  pbs_walltime = "1:00:00"
        String? docker
        """

        //
        // By default PBS copies stdout and stderr from execution host to submission host only
        // after the job is done. This doesn't suit cromwell out-of-the-box, which assumes that
        // once the rc file exists, the output stream files are immediately ready. So we specify
        // "-k oe" to qsub, which keeps the stdout and stderr streams on execution node in
        // $PBS_JOBDIR - by default the user's $HOME which we assume is shared and available to
        // both execution and submission hosts - and insert some shell commands into the generated
        // script to move those files to the locations expected by cromwell /before/ the rc file
        // is created.
        //
        submit = """
        sed -i '/^echo \$? > .*\/rc\.tmp/a \
        jobnum=$(expr match $PBS_JOBID "\\([0-9]\\+\\)") \
        stderr="$PBS_O_HOME/${job_name}.e$jobnum" \
        stdout="$PBS_O_HOME/${job_name}.o$jobnum" \
        mv $stderr ${err} \
        mv $stdout ${out}' \
        ${script}

        qsub \
        -d . \
        -k oe \
        ${"-l nodes=1:ppn=" + pbs_cpu} \
        ${"-l mem=${memory_mb}mb"} \
        ${"-l walltime=" + pbs_walltime} \
        ${"-m ea -M " + pbs_email} \
        ${"-q " + pbs_queue} \
        -N ${job_name} \
        -W umask=0007 \
        ${script}
        """

        submit-docker = """
        sed -i '/^echo \$? > .*\/rc\.tmp/a \
        jobnum=$(expr match $PBS_JOBID "\\([0-9]\\+\\)") \
        stderr="$PBS_O_HOME/${job_name}.e$jobnum" \
        stdout="$PBS_O_HOME/${job_name}.o$jobnum" \
        mv $stderr ${docker_cwd}/execution/stderr \
        mv $stdout ${docker_cwd}/execution/stdout' \
        ${script}

        echo "singularity exec -C -B /EXAMPLE_DATA:/EXAMPLE_DATA:ro,${cwd}:${docker_cwd} /SINGULARITY_IMAGES/${docker}.sif ${job_shell} ${docker_script}" | \
        qsub \
        -d . \
        -k oe \
        ${"-l nodes=1:ppn=" + pbs_cpu} \
        ${"-l mem=${memory_mb}mb"} \
        ${"-l walltime=" + pbs_walltime} \
        ${"-m ea -M " + pbs_email} \
        ${"-q " + pbs_queue} \
        -N ${job_name} \
        -W umask=0007
        """
        

        kill = "qdel ${job_id}"
        check-alive = "qstat ${job_id}"
        job-id-regex = "(\\d+)\\.\\w+\\.\\w+"
        concurrent-job-limit = 25
      }
    }
  }
}
